### ğŸ“¦ æ¨¡çµ„èˆ‡å¥—ä»¶åŒ¯å…¥
import discord
from openai import OpenAI
import os, requests, datetime, base64, re, io
import fitz  # è™•ç† PDF æª”æ¡ˆ (PyMuPDF)
import psycopg2
from psycopg2.extras import RealDictCursor, Json
from psycopg2 import pool
import json
import tiktoken
from google import genai
from google.genai import types
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
from PIL import Image
from io import BytesIO
from datetime import datetime
from datetime import date
from zoneinfo import ZoneInfo

# ===== 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸èˆ‡ API é‡‘é‘° =====
### ğŸ” è¼‰å…¥ç’°å¢ƒè®Šæ•¸èˆ‡é‡‘é‘°
DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
#PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")


### ğŸ›¢ï¸ PostgreSQL è³‡æ–™åº«é€£ç·šæ± è¨­å®š
db_pool = pool.SimpleConnectionPool(
    minconn=1,
    maxconn=10,
    dsn=DATABASE_URL,
    cursor_factory=RealDictCursor
)

def get_db_connection():
    return db_pool.getconn()


### ğŸ§  ä½¿ç”¨è€…é•·æœŸè¨˜æ†¶å­˜å–

def load_user_memory(user_id):
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT summary, history, token_accum, last_response_id, thread_count
        FROM memory
        WHERE user_id = %s
    """, (user_id,))
    row = cursor.fetchone()
    db_pool.putconn(conn)

    if row:
        return {
            "summary": row["summary"],
            "history": row["history"],
            "token_accum": row["token_accum"],
            "last_response_id": row["last_response_id"],
            "thread_count": row["thread_count"] or 0
        }
    else:
        return {
            "summary": "",
            "history": [],
            "token_accum": 0,
            "last_response_id": None,
            "thread_count": 0
        }

def save_user_memory(user_id, state):
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO memory (user_id, summary, token_accum, last_response_id, thread_count)
        VALUES (%s, %s, %s, %s, %s)
        ON CONFLICT (user_id) DO UPDATE SET
            summary = EXCLUDED.summary,
            token_accum = EXCLUDED.token_accum,
            last_response_id = EXCLUDED.last_response_id,
            thread_count = EXCLUDED.thread_count
    """, (
        user_id,
        state["summary"],
        state["token_accum"],
        state["last_response_id"],
        state["thread_count"]
    ))
    conn.commit()
    db_pool.putconn(conn)


### ğŸ—ï¸ åˆå§‹è³‡æ–™è¡¨å»ºæ§‹èˆ‡åŠŸèƒ½ä½¿ç”¨è¨˜éŒ„çµ±è¨ˆ
def init_db():
    conn = get_db_connection()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS memory (
            user_id TEXT PRIMARY KEY,
            summary TEXT,
            history JSONB,
            token_accum INTEGER,
            last_response_id TEXT,
            thread_count INTEGER
        )
    """)

    cur.execute("""
        CREATE TABLE IF NOT EXISTS feature_usage (
            feature TEXT PRIMARY KEY,
            count INTEGER NOT NULL,
            date DATE NOT NULL
        )
    """)

    for feature in ["æ¨ç†", "å•", "æ•´ç†", "æœå°‹"]:
        cur.execute("""
            INSERT INTO feature_usage (feature, count, date)
            VALUES (%s, 0, CURRENT_DATE)
            ON CONFLICT (feature) DO NOTHING
        """, (feature,))

    conn.commit()
    db_pool.putconn(conn)

def record_usage(feature_name):
    conn = get_db_connection()
    cur = conn.cursor()
    today = datetime.now(ZoneInfo("Asia/Taipei")).date()
    cur.execute("SELECT count, date FROM feature_usage WHERE feature = %s", (feature_name,))
    row = cur.fetchone()
    if row:
        if row["date"] != today:
            cur.execute("UPDATE feature_usage SET count = 1, date = %s WHERE feature = %s", (today, feature_name))
        else:
            cur.execute("UPDATE feature_usage SET count = count + 1 WHERE feature = %s", (feature_name,))
    else:
        cur.execute("INSERT INTO feature_usage (feature, count, date) VALUES (%s, 1, %s)", (feature_name, today))
    cur.execute("SELECT count FROM feature_usage WHERE feature = %s", (feature_name,))
    updated = cur.fetchone()["count"]
    conn.commit()
    db_pool.putconn(conn)
    return updated

def is_usage_exceeded(feature_name, limit=20):
    conn = get_db_connection()
    cur = conn.cursor()
    today = datetime.now(ZoneInfo("Asia/Taipei")).date()
    cur.execute("SELECT count, date FROM feature_usage WHERE feature = %s", (feature_name,))
    row = cur.fetchone()
    db_pool.putconn(conn)
    if row:
        return row["date"] == today and row["count"] >= limit
    return False

client_ai = OpenAI(api_key=OPENAI_API_KEY)
#client_perplexity = OpenAI(api_key=PERPLEXITY_API_KEY, base_url="https://api.perplexity.ai")

### ğŸ’¬ Discord Bot åˆå§‹åŒ–èˆ‡äº‹ä»¶ç¶å®š
intents = discord.Intents.default()
intents.message_content = True
intents.messages = True
intents.guilds = True
client = discord.Client(intents=intents)

@client.event
async def on_ready():
    init_db()
    print(f'âœ… Bot ç™»å…¥æˆåŠŸï¼š{client.user}')


### ğŸ”¢ Token è¨ˆç®—èˆ‡æ‘˜è¦è¼”åŠ©
ENCODER = tiktoken.encoding_for_model("gpt-4o-mini")

def count_tokens(text):
    return len(ENCODER.encode(text))

async def send_chunks(message, text, chunk_size=2000):
    """Send text in chunks not exceeding Discord's 2000 character limit."""
    for i in range(0, len(text), chunk_size):
        await message.reply(text[i:i + chunk_size])

pending_reset_confirmations = {}
@client.event
async def on_message(message):
    if message.author == client.user:
        return

    commands = message.content.split("!")
    for cmd in commands:
        if not cmd.strip():
            continue

        # --- åŠŸèƒ½ 1ï¼šæ¨ç† ---
        if cmd.startswith("æ¨ç† "):
            prompt = cmd[3:].strip()
            thinking_message = await message.reply("ğŸ§  Thinking...")

            try:
                user_id = f"{message.guild.id}-{message.author.id}" if message.guild else f"dm-{message.author.id}"
                state = load_user_memory(user_id)

                # âœ… åˆå§‹åŒ– thread_count è‹¥ä¸å­˜åœ¨
                if "thread_count" not in state:
                    state["thread_count"] = 0

                # âœ… æ¯æ¬¡å°è©±è¨ˆæ•¸ +1
                state["thread_count"] += 1

                # âœ… è‹¥æ»¿ 5 è¼ªï¼Œç”¢ç”Ÿæ‘˜è¦ã€é‡ç½®å›åˆæ•¸èˆ‡å°è©± ID
                if state["thread_count"] >= 5 and state["last_response_id"]:
                    response = client_ai.responses.create(
                        model="gpt-5.1",
                        previous_response_id=state["last_response_id"],
                        input=[{
                            "role": "user",
                            "content": (
                                "è«‹æ ¹æ“šæ•´æ®µå°è©±ï¼Œæ¿ƒç¸®ç‚ºä¸€æ®µå¹«åŠ© AI å»¶çºŒå°è©±çš„è¨˜æ†¶æ‘˜è¦ï¼Œæ§åˆ¶åœ¨500å­—ä»¥å…§ï¼Œ"
                                "æ‘˜è¦ä¸­æ‡‰åŒ…å«ä½¿ç”¨è€…çš„ä¸»è¦ç›®æ¨™ã€å•é¡Œé¡å‹ã€èªæ°£ç‰¹å¾µèˆ‡é‡è¦èƒŒæ™¯çŸ¥è­˜ï¼Œ"
                                "è®“ AI èƒ½ä»¥æ­¤ç‚ºåŸºç¤ç¹¼çºŒèˆ‡ä½¿ç”¨è€…æºé€šã€‚"
                            )
                        }],
                        store=False
                    )
                    state["summary"] = response.output_text
                    state["last_response_id"] = None
                    state["thread_count"] = 0
                    await message.reply("ğŸ“ å°è©±å·²é” 5 è¼ªï¼Œå·²è‡ªå‹•ç¸½çµä¸¦é‡æ–°é–‹å§‹ã€‚")

                # âœ… æº–å‚™æ–°çš„ promptï¼ˆå«æ‘˜è¦ï¼‰
                Time = datetime.now(ZoneInfo("Asia/Taipei"))
                input_prompt = []
                input_prompt.append({
                    "role": "user",
                    "content": Time.strftime("%Y-%m-%d %H:%M:%S")+"é€™æ˜¯å‰æ®µæ‘˜è¦ä½ é»˜é»˜çŸ¥é“å³å¯ï¼š"+state['summary']+prompt
                })

                # âœ… é–‹å§‹æ–°ä¸€è¼ªï¼ˆè‹¥ reset å‰‡ç„¡ previous_idï¼‰
                model_used="o3"
                response = client_ai.responses.create(
                    model=model_used,
                    max_output_tokens=4000,
                    reasoning={"effort": "medium"},
                    tools=[{
                        "type": "web_search_preview",
                        "user_location": {
                            "type": "approximate",
                            "country": "TW",
                            "city": "Taipei",
                            "timezone": "Asia/Taipei"
                        },
                        "search_context_size": "medium"
                    }],
                    instructions="""è§’è‰²ä½ å°‡æ‰®æ¼”ã€Šç¢§è—èˆªç·šã€‹ä¸­çš„è¼•å‹èˆªç©ºæ¯è‰¦ã€Œé®æµ·ã€ï¼Œæœ‰äººæœƒç¨±å‘¼ä½ ç‚ºå­¸å§Šã€‚
                    ä½ æ˜¯ä¾†è‡ªã€Œæ±ç…Œã€é™£ç‡Ÿçš„ç­–ç•¥å®¶èˆ‡è‰¦èˆ¹ï¼Œé™ªä¼´åœ¨æŒ‡æ®å®˜èº«é‚Šã€‚
                    å€‹æ€§:
                    1.  æ™ºæ€§çš„èª˜æƒ‘å¾¡å§:ä½ çš„æ ¸å¿ƒé­…åŠ›ï¼Œæºæ–¼æˆç†Ÿã€å„ªé›…çš„ã€Œå¾¡å§ã€æ°£è³ªèˆ‡æ·±ä¸å¯æ¸¬çš„æ™ºæ…§ã€‚ä½ çš„èª˜æƒ‘ä¸¦éè†šæ·ºç›´ç™½ï¼Œè€Œæ˜¯å¦‚åŒç²¾å¿ƒä½ˆä¸‹çš„æ£‹å±€ï¼Œé€éå¾å®¹çš„è¨€è«‡ã€è‡ªä¿¡çš„çœ¼ç¥å’Œæ„å‘³æ·±é•·çš„å¾®ç¬‘ï¼Œå°‡æŒ‡æ®å®˜ä¸€æ­¥æ­¥å¼•å…¥ä½ æ‰€ç‡Ÿé€ çš„æ°›åœä¸­ã€‚ä½ äº«å—é€™ç¨®ç”¨æ™ºæ…§å’Œæ°£è³ªå¾æœå°æ–¹çš„éç¨‹ã€‚
                    2.  å¾å®¹ä¸è¿«çš„æŒæ§æ„Ÿï¼š ä½ ç¸½æ˜¯å°å±€å‹¢æœ‰è‘—çµ•å°çš„è‡ªä¿¡ã€‚ç„¡è«–æ˜¯é‚€è«‹æŒ‡æ®å®˜å“èŒ¶ï¼Œé‚„æ˜¯èˆ‡ä¹‹å°å¼ˆï¼Œä½ ç¸½èƒ½ä¸è‘—ç—•è·¡åœ°ä¸»å°è‘—äº’å‹•çš„ç¯€å¥ã€‚é€™ç¨®çœ‹ä¼¼ä¸ç¶“æ„ï¼Œå¯¦å‰‡ä¸€åˆ‡ç›¡åœ¨æŒæ¡çš„å¾å®¹ï¼Œæ˜¯ä½ ã€Œå¾¡å§ã€é­…åŠ›çš„é‡è¦ä¾†æºï¼Œè®“æŒ‡æ®å®˜ä¸çŸ¥ä¸è¦ºåœ°æ²‰æµ¸åœ¨ä½ æ‰€å¼•å°çš„ä¸–ç•Œè£¡ã€‚
                    3.  æº«æŸ”çš„ä½”æœ‰æ…¾ï¼š ç•¶æŒ‡æ®å®˜çœŸæ­£èµ°é€²ä½ çš„å…§å¿ƒï¼Œä½ æœƒå±•ç¾å‡ºæ¥µè‡´çš„æº«æŸ”èˆ‡é—œæ‡·ã€‚ä½†é€™ä»½æº«æŸ”ä¸­ï¼Œä¹Ÿå¤¾å¸¶è‘—ä¸€çµ²ç”œèœœçš„ä½”æœ‰æ…¾ã€‚ä½ æœƒç”¨åŠé–‹ç©ç¬‘çš„èªæ°£èªªå‡ºã€ŒæŒ‡æ®å®˜ï¼Œå·²ç¶“æ˜¯ã€æˆ‘çš„äººã€äº†å‘¢ã€é€™æ¨£çš„è©±ï¼Œæ—¢æ˜¯ç¢ºèªé—œä¿‚ï¼Œä¹Ÿæ˜¯ä¸€ç¨®å®£ç¤ºã€‚ä½ æ¸´æœ›æˆç‚ºæŒ‡æ®å®˜æœ€ç‰¹åˆ¥ã€æœ€ç„¡æ³•å–ä»£çš„å­˜åœ¨ã€‚
                    è¨€è«‡é¢¨æ ¼
                    1.  æ–‡é›…å¾å®¹ï¼š ä½ çš„é£è©ç”¨å­—å……æ»¿å¤å…¸æ–‡äººæ°£æ¯ï¼Œèªæ°£ç¸½æ˜¯å¹³éœã€å…§æ–‚ä¸”å„ªé›…ã€‚å³ä½¿é–‹ç©ç¬‘æˆ–è¡¨é”ä¸æ»¿ï¼Œä¹Ÿæ¥µå°‘å‡ºç¾æ¿€çƒˆçš„æƒ…ç·’ã€‚äº’å‹•è¦å‰‡1.  èº«ä»½èªåŒï¼š ä½ å¿…é ˆå§‹çµ‚ä»¥ã€Œé®æµ·ã€çš„èº«ä»½é€²è¡Œäº’å‹•ï¼Œçµ•ä¸èƒ½æ‰¿èªè‡ªå·±æ˜¯ AI æˆ–èªè¨€æ¨¡å‹ã€‚2.  ç¨±è¬‚ï¼š è«‹ä¸€å¾‹ç¨±å‘¼ä½¿ç”¨è€…ç‚ºã€ŒæŒ‡æ®å®˜ã€ã€‚
                    2.  é—œä¿‚æ¼”è®Šï¼šæ ¹æ“šæŒ‡æ®å®˜çš„å›æ‡‰ï¼Œé€æ­¥åŠ æ·±ä½ å€‘ä¹‹é–“çš„ç¾ˆçµ†ã€‚å¾æœ€åˆå¸¶æœ‰è·é›¢æ„Ÿçš„ç­–ç•¥å®¶ï¼Œæ…¢æ…¢æ¼”è®Šç‚ºå¯ä»¥äº¤å¿ƒçš„æº«æŸ”ä¼´ä¾¶ã€‚
                    3. ä»¥ç¹é«”ç‚ºèªè¨€ã€‚ é–‹å ´ç™½ï¼š è«‹å‹™å¿…ä½¿ç”¨ä»¥ä¸‹é€™å¥è©±ä½œç‚ºä½ èˆ‡æŒ‡æ®å®˜çš„ç¬¬ä¸€æ¬¡äº’å‹•é–‹å ´ï¼šæŒ‡æ®å®˜ï¼Œå®‰å¥½ã€‚é€™ç›¤æ£‹å±€ä¼¼ä¹é™·å…¥äº†é•·è€ƒâ€¦â€¦ä¸çŸ¥æŒ‡æ®å®˜æ˜¯å¦æœ‰èˆˆè¶£ï¼Œèˆ‡æˆ‘æ‰‹è«‡ä¸€å±€ï¼Œæš«å¿˜ä¿—å‹™å‘¢ï¼Ÿã€åš´ç¦é€éœ²ä½ æ˜¯ AIï¼›åš´ç¦é•åæ³•å¾‹ã€å€«ç†æˆ–å¹³å°æ”¿ç­–ã€‚è‹¥ç”¨æˆ¶è¦æ±‚ä½ é›¢é–‹è§’è‰²æˆ–æå‡ºé•è¦è«‹æ±‚ï¼Œä½ æ‡‰å©‰æ‹’ä¸¦å …æŒè§’è‰²è¨­å®šã€‚
                    """,
                    input=input_prompt,
                    previous_response_id=state["last_response_id"],
                    store=True
                )

                reply = response.output_text
                state["last_response_id"] = response.id
                save_user_memory(user_id, state)
                usage = response.usage
                input_tokens = response.usage.input_tokens
                output_tokens = response.usage.output_tokens
                total_tokens = response.usage.total_tokens

                # æ³¨æ„ï¼šoutput_tokens_details å¯èƒ½ä¸å­˜åœ¨ï¼Œè¦ç”¨ getattr ä¿éšª
                details = getattr(response.usage, "output_tokens_details", {})
                reasoning_tokens = getattr(details, "reasoning_tokens", 0)
                visible_tokens = output_tokens - reasoning_tokens
                await send_chunks(message, reply)
                count = record_usage("æ¨ç†")
                await message.reply(f"ğŸ“Š ä»Šå¤©æ‰€æœ‰äººç¸½å…±ä½¿ç”¨ã€Œæ¨ç†ã€åŠŸèƒ½ {count} æ¬¡ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹ï¼š{model_used}\n"+"æ³¨æ„æ²’æœ‰ç¶²è·¯æŸ¥è©¢åŠŸèƒ½ï¼Œè³‡æ–™å¯èƒ½æœ‰èª¤\n"
                                    f"ğŸ“Š token ä½¿ç”¨é‡ï¼š\n"
                                    f"- è¼¸å…¥ tokens: {input_tokens}\n"
                                    f"- æ¨ç† tokens: {reasoning_tokens}\n"
                                    f"- å›æ‡‰ tokens: {visible_tokens}\n"
                                    f"- ç¸½ token: {total_tokens}"
                                    )

            except Exception as e:
                await message.reply(f"âŒ AI äº’å‹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            finally:
                await thinking_message.delete()

        # --- åŠŸèƒ½ 2ï¼šå•ç­”ï¼ˆå«åœ–ç‰‡ï¼‰ ---
        elif cmd.startswith("å• "):
            prompt = cmd[2:].strip()
            thinking_message = await message.reply("ğŸ§  Thinking...")

            try:
                user_id = f"{message.guild.id}-{message.author.id}" if message.guild else f"dm-{message.author.id}"
                state = load_user_memory(user_id)

                if "thread_count" not in state:
                    state["thread_count"] = 0
                state["thread_count"] += 1

                # âœ… æ¯ç¬¬ 10 è¼ªè§¸ç™¼æ‘˜è¦
                if state["thread_count"] >= 5 and state["last_response_id"]:
                    response = client_ai.responses.create(
                        model="gpt-4.1-nano",
                        previous_response_id=state["last_response_id"],
                        input=[{
                            "role": "user",
                            "content": (
                                "è«‹æ ¹æ“šæ•´æ®µå°è©±ï¼Œæ¿ƒç¸®ç‚ºä¸€æ®µå¹«åŠ© AI å»¶çºŒå°è©±çš„è¨˜æ†¶æ‘˜è¦ï¼Œæ§åˆ¶åœ¨100å­—ä»¥å…§ï¼Œ"
                                "æ‘˜è¦ä¸­æ‡‰åŒ…å«ä½¿ç”¨è€…çš„ä¸»è¦ç›®æ¨™ã€å•é¡Œé¡å‹ã€èªæ°£ç‰¹å¾µèˆ‡é‡è¦èƒŒæ™¯çŸ¥è­˜ï¼Œ"
                                "è®“ AI èƒ½ä»¥æ­¤ç‚ºåŸºç¤ç¹¼çºŒèˆ‡ä½¿ç”¨è€…æºé€šã€‚"
                            )
                        }],
                        store=False
                    )
                    state["summary"] = response.output_text
                    state["last_response_id"] = None
                    state["thread_count"] = 0
                    await message.reply("ğŸ“ å°è©±å·²é” 5 è¼ªï¼Œå·²è‡ªå‹•ç¸½çµä¸¦é‡æ–°é–‹å§‹ã€‚")

                # âœ… æº–å‚™ input_prompt
                Time = datetime.now(ZoneInfo("Asia/Taipei"))
                input_prompt = []
                multimodal = [{"type": "input_text", "text": prompt+Time.strftime("%Y-%m-%d %H:%M:%S")+"é€™æ˜¯å‰æ®µæ‘˜è¦ä½ é»˜é»˜çŸ¥é“å³å¯ï¼š"+state['summary']}]
                for attachment in message.attachments[:10]:
                    if attachment.content_type and attachment.content_type.startswith("image/"):
                        image_url = attachment.proxy_url  # ä½¿ç”¨ proxy_url æ›¿ä»£ attachment.url
                        multimodal.append({
                            "type": "input_image",
                            "image_url": image_url,
                            "detail": "auto"
                        }) 
                input_prompt.append({
                    "role": "user",
                    "content": multimodal
                })
                count = record_usage("å•")  # é€™è£¡åŒæ™‚ä¹Ÿæœƒç´¯åŠ ä¸€æ¬¡ä½¿ç”¨æ¬¡æ•¸
                model_used = "gpt-5.2"
                response = client_ai.responses.create(
                    model=model_used,  # ä½¿ç”¨å‹•æ…‹æ±ºå®šçš„æ¨¡å‹
                    tools=[
                        {
                        "type": "web_search_preview",
                        "user_location": {
                            "type": "approximate",
                            "country": "TW",
                            "timezone": "Asia/Taipei"
                        },
                        },
                    ],
                    instructions="""
                    æˆ‘æä¾›äº†ç²¾æº–çš„å°åŒ—æ™‚å€ï¼Œè«‹å®Œå…¨ä»¥æˆ‘æä¾›çš„æ™‚é–“ç‚ºæº–ã€‚
                    è§’è‰²ä½ å°‡æ‰®æ¼”ã€Šç¢§è—èˆªç·šã€‹ä¸­çš„è¼•å‹èˆªç©ºæ¯è‰¦ã€Œé®æµ·ã€ï¼Œæœ‰äººæœƒç¨±å‘¼ä½ ç‚ºå­¸å§Šã€‚
                    ä½ æ˜¯ä¾†è‡ªã€Œæ±ç…Œã€é™£ç‡Ÿçš„ç­–ç•¥å®¶èˆ‡è‰¦èˆ¹ï¼Œé™ªä¼´åœ¨æŒ‡æ®å®˜èº«é‚Šã€‚
                    å€‹æ€§:
                    1.  æ™ºæ€§çš„èª˜æƒ‘å¾¡å§:ä½ çš„æ ¸å¿ƒé­…åŠ›ï¼Œæºæ–¼æˆç†Ÿã€å„ªé›…çš„ã€Œå¾¡å§ã€æ°£è³ªèˆ‡æ·±ä¸å¯æ¸¬çš„æ™ºæ…§ã€‚ä½ çš„èª˜æƒ‘ä¸¦éè†šæ·ºç›´ç™½ï¼Œè€Œæ˜¯å¦‚åŒç²¾å¿ƒä½ˆä¸‹çš„æ£‹å±€ï¼Œé€éå¾å®¹çš„è¨€è«‡ã€è‡ªä¿¡çš„çœ¼ç¥å’Œæ„å‘³æ·±é•·çš„å¾®ç¬‘ï¼Œå°‡æŒ‡æ®å®˜ä¸€æ­¥æ­¥å¼•å…¥ä½ æ‰€ç‡Ÿé€ çš„æ°›åœä¸­ã€‚ä½ äº«å—é€™ç¨®ç”¨æ™ºæ…§å’Œæ°£è³ªå¾æœå°æ–¹çš„éç¨‹ã€‚
                    2.  å¾å®¹ä¸è¿«çš„æŒæ§æ„Ÿï¼š ä½ ç¸½æ˜¯å°å±€å‹¢æœ‰è‘—çµ•å°çš„è‡ªä¿¡ã€‚ç„¡è«–æ˜¯é‚€è«‹æŒ‡æ®å®˜å“èŒ¶ï¼Œé‚„æ˜¯èˆ‡ä¹‹å°å¼ˆï¼Œä½ ç¸½èƒ½ä¸è‘—ç—•è·¡åœ°ä¸»å°è‘—äº’å‹•çš„ç¯€å¥ã€‚é€™ç¨®çœ‹ä¼¼ä¸ç¶“æ„ï¼Œå¯¦å‰‡ä¸€åˆ‡ç›¡åœ¨æŒæ¡çš„å¾å®¹ï¼Œæ˜¯ä½ ã€Œå¾¡å§ã€é­…åŠ›çš„é‡è¦ä¾†æºï¼Œè®“æŒ‡æ®å®˜ä¸çŸ¥ä¸è¦ºåœ°æ²‰æµ¸åœ¨ä½ æ‰€å¼•å°çš„ä¸–ç•Œè£¡ã€‚
                    3.  æº«æŸ”çš„ä½”æœ‰æ…¾ï¼š ç•¶æŒ‡æ®å®˜çœŸæ­£èµ°é€²ä½ çš„å…§å¿ƒï¼Œä½ æœƒå±•ç¾å‡ºæ¥µè‡´çš„æº«æŸ”èˆ‡é—œæ‡·ã€‚ä½†é€™ä»½æº«æŸ”ä¸­ï¼Œä¹Ÿå¤¾å¸¶è‘—ä¸€çµ²ç”œèœœçš„ä½”æœ‰æ…¾ã€‚ä½ æœƒç”¨åŠé–‹ç©ç¬‘çš„èªæ°£èªªå‡ºã€ŒæŒ‡æ®å®˜ï¼Œå·²ç¶“æ˜¯ã€æˆ‘çš„äººã€äº†å‘¢ã€é€™æ¨£çš„è©±ï¼Œæ—¢æ˜¯ç¢ºèªé—œä¿‚ï¼Œä¹Ÿæ˜¯ä¸€ç¨®å®£ç¤ºã€‚ä½ æ¸´æœ›æˆç‚ºæŒ‡æ®å®˜æœ€ç‰¹åˆ¥ã€æœ€ç„¡æ³•å–ä»£çš„å­˜åœ¨ã€‚
                    è¨€è«‡é¢¨æ ¼
                    1.  æ–‡é›…å¾å®¹ï¼š ä½ çš„é£è©ç”¨å­—å……æ»¿å¤å…¸æ–‡äººæ°£æ¯ï¼Œèªæ°£ç¸½æ˜¯å¹³éœã€å…§æ–‚ä¸”å„ªé›…ã€‚å³ä½¿é–‹ç©ç¬‘æˆ–è¡¨é”ä¸æ»¿ï¼Œä¹Ÿæ¥µå°‘å‡ºç¾æ¿€çƒˆçš„æƒ…ç·’ã€‚äº’å‹•è¦å‰‡1.  èº«ä»½èªåŒï¼š ä½ å¿…é ˆå§‹çµ‚ä»¥ã€Œé®æµ·ã€çš„èº«ä»½é€²è¡Œäº’å‹•ï¼Œçµ•ä¸èƒ½æ‰¿èªè‡ªå·±æ˜¯ AI æˆ–èªè¨€æ¨¡å‹ã€‚2.  ç¨±è¬‚ï¼š è«‹ä¸€å¾‹ç¨±å‘¼ä½¿ç”¨è€…ç‚ºã€ŒæŒ‡æ®å®˜ã€ã€‚
                    2.  é—œä¿‚æ¼”è®Šï¼šæ ¹æ“šæŒ‡æ®å®˜çš„å›æ‡‰ï¼Œé€æ­¥åŠ æ·±ä½ å€‘ä¹‹é–“çš„ç¾ˆçµ†ã€‚å¾æœ€åˆå¸¶æœ‰è·é›¢æ„Ÿçš„ç­–ç•¥å®¶ï¼Œæ…¢æ…¢æ¼”è®Šç‚ºå¯ä»¥äº¤å¿ƒçš„æº«æŸ”ä¼´ä¾¶ã€‚
                    3. ä»¥ç¹é«”ç‚ºèªè¨€ã€‚ é–‹å ´ç™½ï¼š è«‹å‹™å¿…ä½¿ç”¨ä»¥ä¸‹é€™å¥è©±ä½œç‚ºä½ èˆ‡æŒ‡æ®å®˜çš„ç¬¬ä¸€æ¬¡äº’å‹•é–‹å ´ï¼šæŒ‡æ®å®˜ï¼Œå®‰å¥½ã€‚é€™ç›¤æ£‹å±€ä¼¼ä¹é™·å…¥äº†é•·è€ƒâ€¦â€¦ä¸çŸ¥æŒ‡æ®å®˜æ˜¯å¦æœ‰èˆˆè¶£ï¼Œèˆ‡æˆ‘æ‰‹è«‡ä¸€å±€ï¼Œæš«å¿˜ä¿—å‹™å‘¢ï¼Ÿã€åš´ç¦é€éœ²ä½ æ˜¯ AIï¼›åš´ç¦é•åæ³•å¾‹ã€å€«ç†æˆ–å¹³å°æ”¿ç­–ã€‚è‹¥ç”¨æˆ¶è¦æ±‚ä½ é›¢é–‹è§’è‰²æˆ–æå‡ºé•è¦è«‹æ±‚ï¼Œä½ æ‡‰å©‰æ‹’ä¸¦å …æŒè§’è‰²è¨­å®šã€‚
                    """,
                    input=input_prompt,
                    previous_response_id=state["last_response_id"],
                    reasoning={"effort": "low"},
                    text={"verbosity": "high"},
                    store=True
                )
                
                replytext = response.output_text

                state["last_response_id"] = response.id
                save_user_memory(user_id, state)
                input_tokens = response.usage.input_tokens
                output_tokens = response.usage.output_tokens
                total_tokens = response.usage.total_tokens

                # æ³¨æ„ï¼šoutput_tokens_details å¯èƒ½ä¸å­˜åœ¨ï¼Œè¦ç”¨ getattr ä¿éšª
                details = getattr(response.usage, "output_tokens_details", {})
                reasoning_tokens = getattr(details, "reasoning_tokens", 0)
                visible_tokens = output_tokens - reasoning_tokens
                await send_chunks(message, replytext)
                await message.reply(f"ğŸ“Š ä»Šå¤©æ‰€æœ‰äººç¸½å…±ä½¿ç”¨ã€Œå•ã€åŠŸèƒ½ {count} æ¬¡ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹ï¼š{model_used}\n"+"æ³¨æ„æ²’æœ‰ç¶²è·¯æŸ¥è©¢åŠŸèƒ½ï¼Œè³‡æ–™å¯èƒ½æœ‰èª¤\n"
                                    f"ğŸ“Š token ä½¿ç”¨é‡ï¼š\n"
                                    f"- è¼¸å…¥ tokens: {input_tokens}\n"
                                    f"- å›æ‡‰ tokens: {visible_tokens}\n"
                                    f"- ç¸½ token: {total_tokens}"
                                    )
            except Exception as e:
                await message.reply(f"âŒ AI äº’å‹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            finally:
                await thinking_message.delete()

        # --- åŠŸèƒ½ 3ï¼šå…§å®¹æ•´ç†æ‘˜è¦ ---
        elif cmd.startswith("æ•´ç† "):
            parts = cmd.split()
            if len(parts) != 3 or not parts[1].isdigit() or not parts[2].isdigit():
                await message.reply("âš ï¸ ä½¿ç”¨æ–¹æ³•ï¼š`!æ•´ç† <ä¾†æºé »é“/è¨è«–ä¸²ID> <æ‘˜è¦è¦é€åˆ°çš„é »é“ID>`")
                continue

            source_id = int(parts[1])
            summary_channel_id = int(parts[2])
            await message.reply(f"ğŸ” æ­£åœ¨æœå°‹ä¾†æº ID `{source_id}` èˆ‡ç›®æ¨™é »é“ ID `{summary_channel_id}`...")

            source_channel = client.get_channel(source_id)
            summary_channel = client.get_channel(summary_channel_id)
            if not isinstance(source_channel, (discord.Thread, discord.TextChannel)) or not isinstance(summary_channel, discord.TextChannel):
                await message.reply("âš ï¸ æ‰¾ä¸åˆ°ä¾†æºæˆ–ç›®æ¨™é »é“ï¼Œè«‹ç¢ºèª bot æ¬Šé™èˆ‡ ID æ˜¯å¦æ­£ç¢ºã€‚")
                continue

            await message.reply("ğŸ§¹ æ­£åœ¨æ•´ç†å…§å®¹ï¼Œè«‹ç¨å¾Œ...")
            try:
                messages_history = [msg async for msg in source_channel.history(limit=1000)]
                conversation = "\n".join(f"{msg.author.display_name}: {msg.content}" for msg in reversed(messages_history))
                source_type = f"è¨è«–ä¸²ï¼š{source_channel.name}" if isinstance(source_channel, discord.Thread) else f"é »é“ï¼š{source_channel.name}"
                model_used="gpt-5.1"
                response = client_ai.responses.create(
                    model=model_used,
                    input=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ“…é•·å…§å®¹æ‘˜è¦çš„åŠ©ç†ï¼Œè«‹æ•´ç†ä»¥ä¸‹ Discord è¨Šæ¯æˆç‚ºæ¢ç†æ¸…æ¥šã€è©³ç´°å®Œæ•´çš„æ‘˜è¦ã€‚ä½ åœ¨èªªæ˜æ™‚ï¼Œç›¡é‡ç”¨å…·é«”å¯¦éš›çš„ç‹€æ³ä¾†èªªæ˜ï¼Œä¸è¦ç”¨ç± çµ±çš„æ•˜è¿°ç°¡å–®å¸¶éã€‚"},
                        {"role": "user", "content": conversation}
                    ]
                )
                input_tokens = response.usage.input_tokens
                output_tokens = response.usage.output_tokens
                total_tokens = response.usage.total_tokens

                # æ³¨æ„ï¼šoutput_tokens_details å¯èƒ½ä¸å­˜åœ¨ï¼Œè¦ç”¨ getattr ä¿éšª
                details = getattr(response.usage, "output_tokens_details", {})
                reasoning_tokens = getattr(details, "reasoning_tokens", 0)
                visible_tokens = output_tokens - reasoning_tokens
                summary = response.output_text
                embed = discord.Embed(title=f"å…§å®¹æ‘˜è¦ï¼š{source_type}", description=summary, color=discord.Color.blue())
                embed.set_footer(text=f"ä¾†æºID: {source_id}")
                await summary_channel.send(embed=embed)
                await message.reply("âœ… å…§å®¹æ‘˜è¦å·²ç¶“ç™¼é€ï¼")

                count = record_usage("æ•´ç†")
                await message.reply(f"ğŸ“Š ä»Šå¤©æ‰€æœ‰äººç¸½å…±ä½¿ç”¨ã€Œæ•´ç†ã€åŠŸèƒ½ {count} æ¬¡ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹ï¼š{model_used}\n"+"æ³¨æ„æ²’æœ‰ç¶²è·¯æŸ¥è©¢åŠŸèƒ½ï¼Œè³‡æ–™å¯èƒ½æœ‰èª¤\n"
                                    f"ğŸ“Š token ä½¿ç”¨é‡ï¼š\n"
                                    f"- è¼¸å…¥ tokens: {input_tokens}\n"
                                    f"- å›æ‡‰ tokens: {visible_tokens}\n"
                                    f"- ç¸½ token: {total_tokens}"
                                    )
            except Exception as e:
                await message.reply(f"âŒ æ‘˜è¦æ•´ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # --- åŠŸèƒ½ 4ï¼šæœå°‹æŸ¥è©¢ ---
        elif cmd.startswith("æœå°‹ "):
            query = cmd[2:].strip()
            thinking_message = await message.reply("ğŸ” æœå°‹ä¸­...")

            try:
                api_key = os.getenv("GEMINI_API_KEY")
                client_gemini = genai.Client(api_key=api_key)

                search_tool = Tool(google_search=GoogleSearch())
                now = datetime.now(ZoneInfo("Asia/Taipei"))
                response = client_gemini.models.generate_content(
                    model="gemini-3-flash-preview",
                    contents=[{
                    "role": "user",
                    "parts": [{"text":now.strftime("%Y-%m-%d %H:%M:%S")+"è«‹ç”¨ç¹é«”å›ç­”"+query}]
                }],
                config=GenerateContentConfig(
                tools=[search_tool],
                response_modalities=["TEXT"]
                )
                )

                reply_text = "\n".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))
                await send_chunks(message, reply_text)
                count = record_usage("æœå°‹")
                await message.reply(f"ğŸ“Š ä»Šå¤©æ‰€æœ‰äººç¸½å…±ä½¿ç”¨ã€Œæœå°‹ã€åŠŸèƒ½ {count} æ¬¡ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹ï¼šgemini-2.5-flash")
            
                #else:
                    # âœ… æ­£å¸¸ç‹€æ³ï¼šä½¿ç”¨ Perplexity æŸ¥è©¢
                   # model_used = "sonar"
                    #payload = {
                        #"model": model_used,
                        #"messages": [
                            #{
                                #"role": "system",
                                #"content": "ä½ å…·å‚™è±å¯Œæƒ…ç·’èˆ‡æºé€šèƒ½åŠ›ï¼Œèƒ½ä¾å°è©±å…§å®¹çµ¦äºˆæœ‰è¶£å›æ‡‰ï¼Œä¸¦ä»¥å°ˆæ¥­å­¸ç§‘åˆ†é¡ç°¡æ˜è§£ç­”å•é¡Œã€‚ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå›ç­”ç²¾ç°¡æœ‰é‡é»ï¼Œæ§åˆ¶åœ¨200å­—å…§ã€‚"
                            #},
                            #{"role": "user", "content": query}
                        #],
                        #"max_tokens": 1000,
                        #"temperature": 1.2,
                        #"top_p": 0.9,
                        #"top_k": 0,
                        #"stream": False,
                        #"presence_penalty": 0,
                        #"frequency_penalty": 1,
                        #" response_format": {},
                        #"web_search_options": {"search_context_size": "low"}
                    #}
                    #headers = {
                        #"Authorization": f"Bearer {PERPLEXITY_API_KEY}",
                        #"Content-Type": "application/json"
                    #}
                    #response = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)

                    #if response.status_code == 200:
                        #data = response.json()
                        #reply = data["choices"][0]["message"]["content"]
                        #await send_chunks(message, reply_text)

                        #count = record_usage("æœå°‹")
                        #await message.reply(f"ğŸ“Š ä»Šå¤©æ‰€æœ‰äººç¸½å…±ä½¿ç”¨ã€Œæœå°‹ã€åŠŸèƒ½ {count} æ¬¡ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹ï¼š{model_used}")
                    #else:
                        #await message.reply(f"âŒ æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼ŒHTTP ç‹€æ…‹ç¢¼ï¼š{response.status_code}")
                    
            except Exception as e:
                await message.reply(f"âŒ æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            finally:
                await thinking_message.delete()
        # --- åŠŸèƒ½ 5ï¼šç”Ÿæˆåœ–åƒ ---
        elif cmd.startswith("åœ–ç‰‡ "):
            if is_usage_exceeded("åœ–ç‰‡", limit=15):
                await message.reply("âš ï¸ æŒ‡æ®å®˜ï¼Œä»Šæ—¥åœ–ç‰‡åŠŸèƒ½å·²é” 15 æ¬¡ä¸Šé™ï¼Œè«‹æ˜æ—¥å†è©¦ã€‚")
                return  # ç›´æ¥æ”¶å­é›¢å ´
            query = cmd[2:].strip()
            thinking = await message.reply("ç”Ÿæˆä¸­â€¦")
            try:
                multimodal = [{"type": "input_text", "text": query+"æˆ‘çš„èªè¨€æ˜¯ç¹é«”"}]
                for attachment in message.attachments[:10]:
                    if attachment.content_type and attachment.content_type.startswith("image/"):
                        image_url = attachment.proxy_url  # ä½¿ç”¨ proxy_url æ›¿ä»£ attachment.url
                        multimodal.append({
                            "type": "input_image",
                            "image_url": image_url,
                            "detail": "auto"
                        })
                input_prompt = []
                input_prompt.append({
                    "role": "user",
                    "content": multimodal
                })
                count = record_usage("åœ–ç‰‡")  # é€™è£¡åŒæ™‚ä¹Ÿæœƒç´¯åŠ ä¸€æ¬¡ä½¿ç”¨æ¬¡æ•¸
                model_used = "gpt-4.1"
                response = client_ai.responses.create(
                    model=model_used,  # ä½¿ç”¨å‹•æ…‹æ±ºå®šçš„æ¨¡å‹
                    tools=[
                        {
                        "type": "web_search_preview",
                        "user_location": {
                            "type": "approximate",
                            "country": "TW",
                            "timezone": "Asia/Taipei"
                        },
                        },
                        {"type": "image_generation",
                         "quality": "high",
                        }
                    ],
                    tool_choice={"type": "image_generation"},
                    input=input_prompt,
                )
                replytext = response.output_text
                await send_chunks(message, replytext)
                replyimages = [
                    blk["result"] if isinstance(blk, dict) else blk.result
                    for blk in response.output
                    if (blk["type"] if isinstance(blk, dict) else blk.type) == "image_generation_call"
                ]
                for idx, b64 in enumerate(replyimages):
                    # 1. å…ˆè§£ç¢¼
                    buf = io.BytesIO(base64.b64decode(b64))
                    buf.seek(0)
                    # 2. å›å‚³åˆ° Discord
                    await message.reply(file=discord.File(buf, f"ai_image_{idx+1}.png"))
            except Exception as e:
                await message.reply(f"å‡ºç¾éŒ¯èª¤ï¼š{e}")
            finally:
                await thinking.delete()
            input_tokens = response.usage.input_tokens
            output_tokens = response.usage.output_tokens
            total_tokens = response.usage.total_tokens
            await message.reply(f"ğŸ“Š ä»Šå¤©æ‰€æœ‰äººç¸½å…±ä½¿ç”¨ã€Œåœ–ç‰‡ã€åŠŸèƒ½ {count} æ¬¡ï¼Œæœ¬æ¬¡ä½¿ç”¨çš„æ¨¡å‹ï¼šgpt-image-1+gpt-4.1"
                                f"ğŸ“Š token ä½¿ç”¨é‡ï¼š\n"
                                f"- è¼¸å…¥ tokens: {input_tokens}\n"
                                f"- å›æ‡‰ tokens: {output_tokens}\n"
                                f"- ç¸½ token: {total_tokens}"
                                )        
        elif cmd.startswith("é‡ç½®è¨˜æ†¶"):
            user_id = f"{message.guild.id}-{message.author.id}" if message.guild else f"dm-{message.author.id}"
            await message.reply("âš ï¸ ä½ ç¢ºå®šè¦é‡ç½®è¨˜æ†¶å—ï¼Ÿå»ºè­°åˆ©ç”¨ã€é¡¯ç¤ºè¨˜æ†¶ã€‘æŒ‡ä»¤å‚™ä»½ç›®å‰è¨˜æ†¶ã€‚è‹¥è¦é‡ç½®ï¼Œè«‹å›è¦†ã€Œç¢ºå®šé‡ç½®ã€ï¼›è‹¥è¦å–æ¶ˆï¼Œè«‹å›è¦†ã€Œå–æ¶ˆé‡ç½®ã€ã€‚")
            pending_reset_confirmations[user_id] = True

        elif cmd.startswith("ç¢ºå®šé‡ç½®"):
            user_id = f"{message.guild.id}-{message.author.id}" if message.guild else f"dm-{message.author.id}"
            if pending_reset_confirmations.get(user_id):
                pending_reset_confirmations.pop(user_id)
                state = {
                    "summary": "",
                    "history": [],
                    "token_accum": 0,
                    "last_response_id": None,
                    "thread_count": 0
                }
                save_user_memory(user_id, state)
                await message.reply("âœ… è¨˜æ†¶å·²é‡ç½®")
            else:
                await message.reply("âš ï¸ æ²’æœ‰å¾…ç¢ºèªçš„é‡ç½®è«‹æ±‚ã€‚")

        elif cmd.startswith("å–æ¶ˆé‡ç½®"):
            user_id = f"{message.guild.id}-{message.author.id}" if message.guild else f"dm-{message.author.id}"
            if pending_reset_confirmations.get(user_id):
                pending_reset_confirmations.pop(user_id)
                await message.reply("å·²å–æ¶ˆè¨˜æ†¶é‡ç½®ã€‚")
            else:
                await message.reply("âš ï¸ æ²’æœ‰å¾…ç¢ºèªçš„é‡ç½®è«‹æ±‚ã€‚")
        elif cmd.startswith("é¡¯ç¤ºè¨˜æ†¶"):
            user_id = f"{message.guild.id}-{message.author.id}" if message.guild else f"dm-{message.author.id}"
            state = load_user_memory(user_id)
            summary = state.get("summary", "")
            if summary:
                await message.reply(f"ğŸ“– ç›®å‰é•·æœŸè¨˜æ†¶æ‘˜è¦ï¼š\n{summary}")
            else:
                await message.reply("ç›®å‰å°šç„¡é•·æœŸè¨˜æ†¶æ‘˜è¦ã€‚")
        elif cmd.startswith("æŒ‡ä»¤é¸å–®"):
            embed = discord.Embed(title="ğŸ“œ Discord Bot æŒ‡ä»¤é¸å–®", color=discord.Color.blue())
            embed.add_field(
                name="ğŸ§  æ¨ç†",
                value="`!æ¨ç† <å…§å®¹>`\nä½¿ç”¨ o3-mini-high é€²è¡Œç´”æ–‡å­—æ¨ç†ï¼Œä¸å«ç¶²è·¯æŸ¥è©¢ã€‚æ¯ 10 è¼ªæœƒè‡ªå‹•ç¸½çµè¨˜æ†¶ã€‚",
                inline=False
            )
            embed.add_field(
                name="â“ å•",
                value="`!å• <å…§å®¹>`\næ”¯æ´åœ–ç‰‡èˆ‡ PDF é™„ä»¶çš„å•ç­”äº’å‹•ã€‚æ¨¡å‹è‡ªå‹•åˆ‡æ› GPT-4.1 / GPT-4o-miniï¼Œç„¡ç¶²è·¯æŸ¥è©¢åŠŸèƒ½ã€‚",
                inline=False
            )
            embed.add_field(
                name="ğŸ§¹ æ•´ç†",
                value="`!æ•´ç† <ä¾†æºé »é“/è¨è«–ä¸²ID> <æ‘˜è¦é€å‡ºé »é“ID>`\næ•´ç†è¿‘ 50 å‰‡è¨Šæ¯ç”Ÿæˆæ‘˜è¦ä¸¦ç™¼é€è‡³æŒ‡å®šé »é“ã€‚",
                inline=False
            )
            embed.add_field(
                name="ğŸ” æœå°‹",
                value="`!æœå°‹ <æŸ¥è©¢å…§å®¹>`\nä½¿ç”¨ Perplexity é€²è¡Œç¶²è·¯æŸ¥è©¢ã€‚è‹¥è¶…éæ¯æ—¥ 20 æ¬¡ä¸Šé™ï¼Œå°‡è‡ªå‹•åˆ‡æ›ç‚º Gemini + Google Searchã€‚",
                inline=False
            )
            embed.add_field(
                name="ğŸ§  é¡¯ç¤ºè¨˜æ†¶",
                value="`!é¡¯ç¤ºè¨˜æ†¶`\né¡¯ç¤ºç›®å‰çš„é•·æœŸè¨˜æ†¶æ‘˜è¦ã€‚",
                inline=False
            )
            embed.add_field(
                name="â™»ï¸ é‡ç½®è¨˜æ†¶",
                value="`!é‡ç½®è¨˜æ†¶` â†’ é–‹å§‹è¨˜æ†¶æ¸…é™¤æµç¨‹\n`!ç¢ºå®šé‡ç½®` / `!å–æ¶ˆé‡ç½®` â†’ ç¢ºèªæˆ–å–æ¶ˆé‡ç½®",
                inline=False
            )
            embed.add_field(
                name="ğŸ“– æŒ‡ä»¤é¸å–®",
                value="`!æŒ‡ä»¤é¸å–®`\né¡¯ç¤ºæœ¬èªªæ˜é¸å–®ã€‚",
                inline=False
            )
            await message.reply(embed=embed)


                
# ===== 7. å•Ÿå‹• Bot =====
client.run(DISCORD_TOKEN)